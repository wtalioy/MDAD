# CMAD: Comprehensive Multi-domain Audio Deepfake Benchmark

CMAD (Comprehensive Multi-domain Audio Deepfake benchmark) is a large-scale benchmark for evaluating audio deepfake detection methods across diverse domains and scenarios. This repository provides evaluation tools, baseline models, and comprehensive datasets for advancing research in audio deepfake detection.

## ğŸ”¥ Features

- **Multi-domain Coverage**: 13 diverse datasets spanning different audio domains (news, interviews, movies, audiobooks, etc.)
- **Comprehensive Evaluation**: Support for both cross-domain and in-domain evaluation scenarios
- **State-of-the-art Baselines**: 6 advanced audio deepfake detection models
- **Rich Audio Content**: Over 422 hours of audio data with balanced real and fake samples
- **Flexible Framework**: Easy-to-use evaluation pipeline with modular design

## ğŸ“Š Dataset Overview

CMAD includes 13 diverse datasets across multiple domains:

| Dataset | Domain | Real Duration | Fake Duration | Total Duration | Real Files | Fake Files |
|---------|--------|---------------|---------------|----------------|------------|------------|
| **Audiobook** | Audiobooks | 19h 24m | 27h 48m | 47h 13m | 9,425 | 13,612 |
| **Emotional** | Emotional Speech | 29h 4m | 29h 51m | 58h 55m | 35,000 | 36,000 |
| **Interview** | Interviews | 27h 20m | 31h 52m | 59h 13m | 12,095 | 12,096 |
| **Movie** | Movie Dialogues | 8h 6m | 11h 51m | 19h 57m | 9,167 | 9,115 |
| **News** | News Reports | 20h 42m | 19h 9m | 39h 52m | 4,910 | 4,910 |
| **NoisySpeech** | Noisy Environments | 0h | 18h 0m | 18h 0m | 0 | 7,505 |
| **PartialFake** | Partial Synthesis | 0h | 22h 57m | 22h 57m | 0 | 7,281 |
| **PhoneCall** | Phone Conversations | 12h 3m | 14h 47m | 26h 51m | 8,268 | 8,237 |
| **Podcast** | Podcasts | 22h 42m | 24h 16m | 46h 58m | 4,948 | 4,942 |
| **PublicFigure** | Public Figures | 15h 17m | 14h 45m | 30h 2m | 9,931 | 7,631 |
| **PublicSpeech** | Public Speeches | 26h 10m | 26h 35m | 52h 45m | 9,651 | 9,651 |

**Total**: 180h 49m real + 241h 51m fake = **422h 41m** (103,395 real + 120,980 fake files)

## ğŸ—ï¸ Installation

### Prerequisites
- Python 3.8+
- CUDA-compatible GPU (recommended)
- 64GB+ free disk space for full dataset

### Setup

1. **Clone the repository**:
```bash
git clone https://github.com/wtalioy/CMAD.git
cd CMAD
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Download datasets**: 
Place your dataset files in the `data/` directory following this structure:
```
data/
â”œâ”€â”€ Audiobook/
â”œâ”€â”€ Emotional/
â”œâ”€â”€ Interview/
â”œâ”€â”€ Movie/
â”œâ”€â”€ News/
â”œâ”€â”€ NoisySpeech/
â”œâ”€â”€ PartialFake/
â”œâ”€â”€ PhoneCall/
â”œâ”€â”€ Podcast/
â”œâ”€â”€ PublicFigure/
â””â”€â”€ PublicSpeech/
```

## ğŸš€ Quick Start

### Basic Evaluation

Evaluate a baseline model on multiple datasets:

```bash
cd src/eval
python main.py --baseline aasist --dataset interview publicspeech --mode cross --metric eer
```

### Cross-domain Evaluation

Evaluate multiple baselines across domains (no training required):

```bash
python main.py \
    --baseline aasist rawnet2 res-tssdnet \
    --dataset phonecall publicspeech interview \
    --mode cross \
    --metric eer
```

### In-domain Evaluation

Train and evaluate on the same domain:

```bash
python main.py \
    --baseline aasist \
    --dataset interview \
    --mode in \
    --metric eer
```

### Training Only

Train a model without evaluation:

```bash
python main.py \
    --baseline aasist \
    --dataset interview \
    --mode in \
    --train_only
```

### Evaluation Only

Evaluate a pre-trained model:

```bash
python main.py \
    --baseline aasist \
    --dataset interview \
    --mode in \
    --eval_only
```

## ğŸ¯ Available Baselines

CMAD includes 6 state-of-the-art audio deepfake detection models:

| Baseline | Description | Paper |
|----------|-------------|-------|
| **AASIST** | Audio Anti-Spoofing using Integrated Spectro-Temporal Graph Attention Networks | [ICASSP 2021](https://arxiv.org/abs/2110.01200) |
| **AASIST-L** | Large variant of AASIST | [ICASSP 2021](https://arxiv.org/abs/2110.01200) |
| **RawNet2** | End-to-end anti-spoofing using raw waveforms | [Interspeech 2020](https://arxiv.org/abs/2011.01108) |
| **Res-TSSDNet** | Residual Time-frequency Squeeze-and-Scale Network | [ICASSP 2023](https://arxiv.org/abs/2210.06694) |
| **Inc-TSSDNet** | Inception Time-frequency Squeeze-and-Scale Network | [ICASSP 2023](https://arxiv.org/abs/2210.06694) |
| **RawGAT-ST** | Graph Attention Networks with Spectro-Temporal features | [ICASSP 2022](https://arxiv.org/abs/2203.06028) |
| **ARDetect** | Maximum Mean Discrepancy based detection | [ICLR 2024](https://arxiv.org/abs/2309.15603) |

## ğŸ“ˆ Evaluation Metrics

CMAD supports the following evaluation metrics:

- **EER** (Equal Error Rate): Primary metric for audio deepfake detection

## ğŸ”§ Advanced Usage

### Custom Dataset

To add a new dataset, create a class inheriting from `BaseDataset`:

```python
from cmad_datasets.base import BaseDataset

class MyDataset(BaseDataset):
    def __init__(self, data_dir=None, *args, **kwargs):
        super().__init__(os.path.join(data_dir or "data", "MyDataset"), *args, **kwargs)
        self.name = "MyDataset"
```

### Custom Baseline

To add a new baseline model, inherit from the `Baseline` class:

```python
from baselines.base import Baseline

class MyBaseline(Baseline):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.name = "MyBaseline"
        self.supported_metrics = ["eer", "acc"]
    
    def evaluate(self, data, labels, metrics, **kwargs):
        # Implementation
        pass
```

### Configuration

Model configurations are stored in `src/eval/baselines/{model}/config/`:
- `model.yaml`: Model architecture configuration
- `train_default.yaml`: Default training configuration
- `train_{dataset}.yaml`: Dataset-specific training configuration

## ğŸ“ Repository Structure

```
CMAD/
â”œâ”€â”€ data/                          # Dataset directory
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â”œâ”€â”€ main.py               # Main evaluation script
â”‚   â”‚   â”œâ”€â”€ config.py             # Configuration definitions
â”‚   â”‚   â”œâ”€â”€ baselines/            # Baseline model implementations
â”‚   â”‚   â”‚   â”œâ”€â”€ aasist/
â”‚   â”‚   â”‚   â”œâ”€â”€ rawnet2/
â”‚   â”‚   â”‚   â”œâ”€â”€ TSSDNet/
â”‚   â”‚   â”‚   â”œâ”€â”€ RawGAT_ST/
â”‚   â”‚   â”‚   â””â”€â”€ ardetect/
â”‚   â”‚   â””â”€â”€ cmad_datasets/        # Dataset loading implementations
â”‚   â”œâ”€â”€ generation/               # Audio generation tools
â”‚   â”‚   â”œâ”€â”€ main.py               # Main generation script
â”‚   â”‚   â”œâ”€â”€ models/               # TTS and Voice Conversion models
â”‚   â”‚   â”‚   â”œâ”€â”€ tts/              # Text-to-Speech models
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ bark.py       # Bark TTS model
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ elevenlabs_tts.py # ElevenLabs TTS API
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ gemini_tts.py # Gemini TTS model
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ gpt4omini_tts.py # GPT-4o Mini TTS
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ melotts.py    # MeloTTS model
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ tacotron2.py  # Tacotron2 model
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ vits/         # VITS model implementation
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ xttsv2.py     # XTTS v2 model
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ yourtts.py    # YourTTS model
â”‚   â”‚   â”‚   â””â”€â”€ vc/               # Voice Conversion models
â”‚   â”‚   â”‚       â”œâ”€â”€ freevc.py     # FreeVC model
â”‚   â”‚   â”‚       â”œâ”€â”€ knnvc.py      # kNN-VC model
â”‚   â”‚   â”‚       â””â”€â”€ openvoice.py  # OpenVoice model
â”‚   â”‚   â”œâ”€â”€ noise/                # Background noise samples
â”‚   â”‚   â”œâ”€â”€ samples/              # Sample audio files
â”‚   â”‚   â”‚   â”œâ”€â”€ en.wav            # English sample
â”‚   â”‚   â”‚   â””â”€â”€ zh-cn.wav         # Chinese sample
â”‚   â”‚   â””â”€â”€ transcription/        # Speech-to-text tools
â”‚   â”‚       â”œâ”€â”€ parakeet.py       # Parakeet transcription
â”‚   â”‚       â””â”€â”€ voxtral.py        # Voxtral transcription
â”‚   â””â”€â”€ utils/                    # Utility functions
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This file
```

## ğŸ“Š Logging and Results

Evaluation results are automatically logged to:
- Console output with detailed metrics
- `logs/eval.log`: Comprehensive evaluation logs with rotation

Example output:
```
(AASIST on Interview) eer: 0.1234
(RawNet2 on PublicSpeech) eer: 0.2345
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“ Citation

If you use CMAD in your research, please cite:

```bibtex
@inproceedings{cmad2024,
  title={CMAD: Comprehensive Multi-domain Audio Deepfake Benchmark},
  author={Your Name and Collaborators},
  booktitle={Conference Name},
  year={2024}
}
```

## ğŸ”— Links

- **Dataset on Hugging Face**: [Lioy/CMAD](https://huggingface.co/datasets/Lioy/CMAD)
- **Paper**: [Coming Soon]
- **Demo**: [Coming Soon]

---

<div align="center">
Made with â¤ï¸ for advancing audio deepfake detection research
</div>
